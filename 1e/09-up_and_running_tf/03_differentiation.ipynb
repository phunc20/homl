{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-distinction",
   "metadata": {},
   "source": [
    "# Pratical Example: `california_housing`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-opposition",
   "metadata": {},
   "source": [
    "## Normal equation (from linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "economic-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/phunc20/.config/miniconda3/envs/homl1e/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facial-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacterial-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape((-1,1)), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta_other = tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), tf.matmul(XT, y))\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value, theta_other_value = sess.run((theta, theta_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composed-midwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.62395554e+01],\n",
       "       [ 4.38410223e-01],\n",
       "       [ 9.56030935e-03],\n",
       "       [-1.09072715e-01],\n",
       "       [ 6.50818944e-01],\n",
       "       [-3.61849015e-06],\n",
       "       [-3.80502851e-03],\n",
       "       [-4.14003015e-01],\n",
       "       [-4.26401258e-01]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unique-behalf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.62382812e+01],\n",
       "       [ 4.38407898e-01],\n",
       "       [ 9.55867767e-03],\n",
       "       [-1.09069824e-01],\n",
       "       [ 6.50848389e-01],\n",
       "       [-3.61911952e-06],\n",
       "       [-3.80516052e-03],\n",
       "       [-4.14001465e-01],\n",
       "       [-4.26452637e-01]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_other_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "russian-wings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(theta_other_value, theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-siemens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2741089e-03],\n",
       "       [-2.3245811e-06],\n",
       "       [-1.6316772e-06],\n",
       "       [ 2.8908253e-06],\n",
       "       [ 2.9444695e-05],\n",
       "       [-6.2937033e-10],\n",
       "       [-1.3201497e-07],\n",
       "       [ 1.5497208e-06],\n",
       "       [-5.1379204e-05]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_other_value - theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smart-strike",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(theta_other_value, theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mounted-finance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(theta_other_value, theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integrated-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(theta_other_value, theta_value, rtol=1e-3, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "taken-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(theta_other_value, theta_value, rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "appropriate-batman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(theta_other_value, theta_value, rtol=1e-2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "judicial-armstrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-sigma",
   "metadata": {},
   "source": [
    "### `np.array_equal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "photographic-marble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equiv(theta_other_value, theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "convertible-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_equal in module numpy:\n",
      "\n",
      "array_equal(a1, a2, equal_nan=False)\n",
      "    True if two arrays have the same shape and elements, False otherwise.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a1, a2 : array_like\n",
      "        Input arrays.\n",
      "    equal_nan : bool\n",
      "        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n",
      "        complex, values will be considered equal if either the real or the\n",
      "        imaginary component of a given value is ``nan``.\n",
      "    \n",
      "        .. versionadded:: 1.19.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    b : bool\n",
      "        Returns True if the arrays are equal.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    allclose: Returns True if two arrays are element-wise equal within a\n",
      "              tolerance.\n",
      "    array_equiv: Returns True if input arrays are shape consistent and all\n",
      "                 elements equal.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.array_equal([1, 2], [1, 2])\n",
      "    True\n",
      "    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n",
      "    True\n",
      "    >>> np.array_equal([1, 2], [1, 2, 3])\n",
      "    False\n",
      "    >>> np.array_equal([1, 2], [1, 4])\n",
      "    False\n",
      "    >>> a = np.array([1, np.nan])\n",
      "    >>> np.array_equal(a, a)\n",
      "    False\n",
      "    >>> np.array_equal(a, a, equal_nan=True)\n",
      "    True\n",
      "    \n",
      "    When ``equal_nan`` is True, complex values with nan components are\n",
      "    considered equal if either the real *or* the imaginary components are nan.\n",
      "    \n",
      "    >>> a = np.array([1 + 1j])\n",
      "    >>> b = a.copy()\n",
      "    >>> a.real = np.nan\n",
      "    >>> b.imag = np.nan\n",
      "    >>> np.array_equal(a, b, equal_nan=True)\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.array_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "convertible-building",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_equiv in module numpy:\n",
      "\n",
      "array_equiv(a1, a2)\n",
      "    Returns True if input arrays are shape consistent and all elements equal.\n",
      "    \n",
      "    Shape consistent means they are either the same shape, or one input array\n",
      "    can be broadcasted to create the same shape as the other one.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a1, a2 : array_like\n",
      "        Input arrays.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : bool\n",
      "        True if equivalent, False otherwise.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.array_equiv([1, 2], [1, 2])\n",
      "    True\n",
      "    >>> np.array_equiv([1, 2], [1, 3])\n",
      "    False\n",
      "    \n",
      "    Showing the shape equivalence:\n",
      "    \n",
      "    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n",
      "    True\n",
      "    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n",
      "    False\n",
      "    \n",
      "    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n",
      "    False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.array_equiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "purple-occupation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function allclose in module numpy:\n",
      "\n",
      "allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)\n",
      "    Returns True if two arrays are element-wise equal within a tolerance.\n",
      "    \n",
      "    The tolerance values are positive, typically very small numbers.  The\n",
      "    relative difference (`rtol` * abs(`b`)) and the absolute difference\n",
      "    `atol` are added together to compare against the absolute difference\n",
      "    between `a` and `b`.\n",
      "    \n",
      "    NaNs are treated as equal if they are in the same place and if\n",
      "    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n",
      "    place and of the same sign in both arrays.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a, b : array_like\n",
      "        Input arrays to compare.\n",
      "    rtol : float\n",
      "        The relative tolerance parameter (see Notes).\n",
      "    atol : float\n",
      "        The absolute tolerance parameter (see Notes).\n",
      "    equal_nan : bool\n",
      "        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n",
      "        considered equal to NaN's in `b` in the output array.\n",
      "    \n",
      "        .. versionadded:: 1.10.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    allclose : bool\n",
      "        Returns True if the two arrays are equal within the given\n",
      "        tolerance; False otherwise.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    isclose, all, any, equal\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    If the following equation is element-wise True, then allclose returns\n",
      "    True.\n",
      "    \n",
      "     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n",
      "    \n",
      "    The above equation is not symmetric in `a` and `b`, so that\n",
      "    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n",
      "    some rare cases.\n",
      "    \n",
      "    The comparison of `a` and `b` uses standard broadcasting, which\n",
      "    means that `a` and `b` need not have the same shape in order for\n",
      "    ``allclose(a, b)`` to evaluate to True.  The same is true for\n",
      "    `equal` but not `array_equal`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n",
      "    False\n",
      "    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n",
      "    True\n",
      "    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n",
      "    False\n",
      "    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n",
      "    False\n",
      "    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.allclose)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "guided-scheme",
   "metadata": {},
   "source": [
    "absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-creek",
   "metadata": {},
   "source": [
    "## Gradient descent manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-throw",
   "metadata": {},
   "source": [
    "### w/o normalizing the data\n",
    "**(?1)** Do we need `tf.reset_default_graph()` to avoid mixing together what we are about to do with what we just did?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dramatic-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 2855737982189568.0000\n",
      "(epoch 0100) MSE = nan\n",
      "(epoch 0200) MSE = nan\n",
      "(epoch 0300) MSE = nan\n",
      "(epoch 0400) MSE = nan\n",
      "(epoch 0500) MSE = nan\n",
      "(epoch 0600) MSE = nan\n",
      "(epoch 0700) MSE = nan\n",
      "(epoch 0800) MSE = nan\n",
      "(epoch 0900) MSE = nan\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-turtle",
   "metadata": {},
   "source": [
    "**(?2)** The MSE is not decreasing. What's happened?<br>\n",
    "Is it because we should have done a `tf.reset_default_graph()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "psychological-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "detailed-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 4466536207089664.0000\n",
      "(epoch 0100) MSE = nan\n",
      "(epoch 0200) MSE = nan\n",
      "(epoch 0300) MSE = nan\n",
      "(epoch 0400) MSE = nan\n",
      "(epoch 0500) MSE = nan\n",
      "(epoch 0600) MSE = nan\n",
      "(epoch 0700) MSE = nan\n",
      "(epoch 0800) MSE = nan\n",
      "(epoch 0900) MSE = nan\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-double",
   "metadata": {},
   "source": [
    "I've double-checked: There seems to be no typo in the code, at least no significant diff from the code in the book. Was it due to lack of normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-sullivan",
   "metadata": {},
   "source": [
    "### w/ data normalized\n",
    "- `sklearn`\n",
    "- `tf`\n",
    "- `np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "universal-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "useful-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 6.1480\n",
      "(epoch 0100) MSE = 0.8191\n",
      "(epoch 0200) MSE = 0.6455\n",
      "(epoch 0300) MSE = 0.6074\n",
      "(epoch 0400) MSE = 0.5841\n",
      "(epoch 0500) MSE = 0.5675\n",
      "(epoch 0600) MSE = 0.5555\n",
      "(epoch 0700) MSE = 0.5469\n",
      "(epoch 0800) MSE = 0.5406\n",
      "(epoch 0900) MSE = 0.5361\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-power",
   "metadata": {},
   "source": [
    "**(R2)** Yes, it seems that normalizing the data in this case is of crucial importance.\n",
    "\n",
    "**(?3)** In the book (p.237 on top) it says that training w/o first normalizing the data only slows down the training. Let's train the unnormalized data with a prolonged `n_epochs`, say `10_000`, to see if that is true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "treated-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 1150065271898112.0000\n",
      "(epoch 0100) MSE = nan\n",
      "(epoch 0200) MSE = nan\n",
      "(epoch 0300) MSE = nan\n",
      "(epoch 0400) MSE = nan\n",
      "(epoch 0500) MSE = nan\n",
      "(epoch 0600) MSE = nan\n",
      "(epoch 0700) MSE = nan\n",
      "(epoch 0800) MSE = nan\n",
      "(epoch 0900) MSE = nan\n",
      "(epoch 1000) MSE = nan\n",
      "(epoch 1100) MSE = nan\n",
      "(epoch 1200) MSE = nan\n",
      "(epoch 1300) MSE = nan\n",
      "(epoch 1400) MSE = nan\n",
      "(epoch 1500) MSE = nan\n",
      "(epoch 1600) MSE = nan\n",
      "(epoch 1700) MSE = nan\n",
      "(epoch 1800) MSE = nan\n",
      "(epoch 1900) MSE = nan\n",
      "(epoch 2000) MSE = nan\n",
      "(epoch 2100) MSE = nan\n",
      "(epoch 2200) MSE = nan\n",
      "(epoch 2300) MSE = nan\n",
      "(epoch 2400) MSE = nan\n",
      "(epoch 2500) MSE = nan\n",
      "(epoch 2600) MSE = nan\n",
      "(epoch 2700) MSE = nan\n",
      "(epoch 2800) MSE = nan\n",
      "(epoch 2900) MSE = nan\n",
      "(epoch 3000) MSE = nan\n",
      "(epoch 3100) MSE = nan\n",
      "(epoch 3200) MSE = nan\n",
      "(epoch 3300) MSE = nan\n",
      "(epoch 3400) MSE = nan\n",
      "(epoch 3500) MSE = nan\n",
      "(epoch 3600) MSE = nan\n",
      "(epoch 3700) MSE = nan\n",
      "(epoch 3800) MSE = nan\n",
      "(epoch 3900) MSE = nan\n",
      "(epoch 4000) MSE = nan\n",
      "(epoch 4100) MSE = nan\n",
      "(epoch 4200) MSE = nan\n",
      "(epoch 4300) MSE = nan\n",
      "(epoch 4400) MSE = nan\n",
      "(epoch 4500) MSE = nan\n",
      "(epoch 4600) MSE = nan\n",
      "(epoch 4700) MSE = nan\n",
      "(epoch 4800) MSE = nan\n",
      "(epoch 4900) MSE = nan\n",
      "(epoch 5000) MSE = nan\n",
      "(epoch 5100) MSE = nan\n",
      "(epoch 5200) MSE = nan\n",
      "(epoch 5300) MSE = nan\n",
      "(epoch 5400) MSE = nan\n",
      "(epoch 5500) MSE = nan\n",
      "(epoch 5600) MSE = nan\n",
      "(epoch 5700) MSE = nan\n",
      "(epoch 5800) MSE = nan\n",
      "(epoch 5900) MSE = nan\n",
      "(epoch 6000) MSE = nan\n",
      "(epoch 6100) MSE = nan\n",
      "(epoch 6200) MSE = nan\n",
      "(epoch 6300) MSE = nan\n",
      "(epoch 6400) MSE = nan\n",
      "(epoch 6500) MSE = nan\n",
      "(epoch 6600) MSE = nan\n",
      "(epoch 6700) MSE = nan\n",
      "(epoch 6800) MSE = nan\n",
      "(epoch 6900) MSE = nan\n",
      "(epoch 7000) MSE = nan\n",
      "(epoch 7100) MSE = nan\n",
      "(epoch 7200) MSE = nan\n",
      "(epoch 7300) MSE = nan\n",
      "(epoch 7400) MSE = nan\n",
      "(epoch 7500) MSE = nan\n",
      "(epoch 7600) MSE = nan\n",
      "(epoch 7700) MSE = nan\n",
      "(epoch 7800) MSE = nan\n",
      "(epoch 7900) MSE = nan\n",
      "(epoch 8000) MSE = nan\n",
      "(epoch 8100) MSE = nan\n",
      "(epoch 8200) MSE = nan\n",
      "(epoch 8300) MSE = nan\n",
      "(epoch 8400) MSE = nan\n",
      "(epoch 8500) MSE = nan\n",
      "(epoch 8600) MSE = nan\n",
      "(epoch 8700) MSE = nan\n",
      "(epoch 8800) MSE = nan\n",
      "(epoch 8900) MSE = nan\n",
      "(epoch 9000) MSE = nan\n",
      "(epoch 9100) MSE = nan\n",
      "(epoch 9200) MSE = nan\n",
      "(epoch 9300) MSE = nan\n",
      "(epoch 9400) MSE = nan\n",
      "(epoch 9500) MSE = nan\n",
      "(epoch 9600) MSE = nan\n",
      "(epoch 9700) MSE = nan\n",
      "(epoch 9800) MSE = nan\n",
      "(epoch 9900) MSE = nan\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 10_000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-contact",
   "metadata": {},
   "source": [
    "I even tried `n_epochs = 100_000`, we still get `MSE = nan` right after the first iteration. **(R3)** It seems that no normalization not only slows down the training, but sometimes it won't lead to a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-connecticut",
   "metadata": {},
   "source": [
    "**(?4)** This is really intriguing. Had we used `MinMaxScaler` instead of `StandardScaler`, would convergence\n",
    "at least take place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "built-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaled_housing_data = minmax_scaler.fit_transform(housing.data)\n",
    "minmax_scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), minmax_scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dimensional-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quick-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 6.8732\n",
      "(epoch 0100) MSE = 1.2738\n",
      "(epoch 0200) MSE = 1.2239\n",
      "(epoch 0300) MSE = 1.1833\n",
      "(epoch 0400) MSE = 1.1459\n",
      "(epoch 0500) MSE = 1.1114\n",
      "(epoch 0600) MSE = 1.0795\n",
      "(epoch 0700) MSE = 1.0497\n",
      "(epoch 0800) MSE = 1.0221\n",
      "(epoch 0900) MSE = 0.9962\n",
      "(epoch 1000) MSE = 0.9721\n",
      "(epoch 1100) MSE = 0.9496\n",
      "(epoch 1200) MSE = 0.9285\n",
      "(epoch 1300) MSE = 0.9088\n",
      "(epoch 1400) MSE = 0.8903\n",
      "(epoch 1500) MSE = 0.8730\n",
      "(epoch 1600) MSE = 0.8568\n",
      "(epoch 1700) MSE = 0.8416\n",
      "(epoch 1800) MSE = 0.8274\n",
      "(epoch 1900) MSE = 0.8140\n",
      "(epoch 2000) MSE = 0.8014\n",
      "(epoch 2100) MSE = 0.7897\n",
      "(epoch 2200) MSE = 0.7786\n",
      "(epoch 2300) MSE = 0.7683\n",
      "(epoch 2400) MSE = 0.7585\n",
      "(epoch 2500) MSE = 0.7494\n",
      "(epoch 2600) MSE = 0.7408\n",
      "(epoch 2700) MSE = 0.7327\n",
      "(epoch 2800) MSE = 0.7251\n",
      "(epoch 2900) MSE = 0.7180\n",
      "(epoch 3000) MSE = 0.7113\n",
      "(epoch 3100) MSE = 0.7050\n",
      "(epoch 3200) MSE = 0.6990\n",
      "(epoch 3300) MSE = 0.6934\n",
      "(epoch 3400) MSE = 0.6881\n",
      "(epoch 3500) MSE = 0.6832\n",
      "(epoch 3600) MSE = 0.6785\n",
      "(epoch 3700) MSE = 0.6741\n",
      "(epoch 3800) MSE = 0.6699\n",
      "(epoch 3900) MSE = 0.6659\n",
      "(epoch 4000) MSE = 0.6622\n",
      "(epoch 4100) MSE = 0.6587\n",
      "(epoch 4200) MSE = 0.6554\n",
      "(epoch 4300) MSE = 0.6523\n",
      "(epoch 4400) MSE = 0.6493\n",
      "(epoch 4500) MSE = 0.6465\n",
      "(epoch 4600) MSE = 0.6438\n",
      "(epoch 4700) MSE = 0.6413\n",
      "(epoch 4800) MSE = 0.6389\n",
      "(epoch 4900) MSE = 0.6366\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(minmax_scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-rental",
   "metadata": {},
   "source": [
    "**(R4)** It seems that `MinMaxScaler` also allows convergence, but the convergence is much slower, compared\n",
    "to using `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-context",
   "metadata": {},
   "source": [
    "Don't forget that we still need to verify the validity of the formula `gradients = 2/m * tf.matmul(tf.transpose(X), error)`\n",
    "\n",
    "- `theta.shape` equals `(n+1, 1)`\n",
    "  - For `gradients` to be correct, at least `gradients.shape` should be **the same**: `error.shape` equals `(m, 1)`, `X.shape` being `(m, n+1)` $\\implies$ `tf.matmul(tf.transpose(X), error)` equal to `(n+1, 1)`.\n",
    "- It's already quite lengthy, this notebook. Let's edit the derivation of the formula in a separate pdf file named `grad_mse.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-medium",
   "metadata": {},
   "source": [
    "### Autodiff\n",
    "With only one line modified from the code of manual computing `gradients`, we can utilize autodiff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-message",
   "metadata": {},
   "source": [
    "BTW, I just came up with a way to see the effect of `tf.reset_default_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "actual-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "intense-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainable_variables', 'variables']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "hairy-target",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'X' type=Const>,\n",
       " <tf.Operation 'y' type=Const>,\n",
       " <tf.Operation 'random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'random_uniform/min' type=Const>,\n",
       " <tf.Operation 'random_uniform/max' type=Const>,\n",
       " <tf.Operation 'random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'random_uniform' type=Add>,\n",
       " <tf.Operation 'theta' type=VariableV2>,\n",
       " <tf.Operation 'theta/Assign' type=Assign>,\n",
       " <tf.Operation 'theta/read' type=Identity>,\n",
       " <tf.Operation 'predictions' type=MatMul>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'Square' type=Square>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'mse' type=Mean>,\n",
       " <tf.Operation 'transpose/perm' type=Const>,\n",
       " <tf.Operation 'transpose' type=Transpose>,\n",
       " <tf.Operation 'MatMul' type=MatMul>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'mul_1/x' type=Const>,\n",
       " <tf.Operation 'mul_1' type=Mul>,\n",
       " <tf.Operation 'sub_1' type=Sub>,\n",
       " <tf.Operation 'Assign' type=Assign>,\n",
       " <tf.Operation 'init' type=NoOp>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "regulation-mobility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'theta:0' shape=(9, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_collection(\"variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "drawn-enlargement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'theta:0' shape=(9, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_collection(\"trainable_variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-champagne",
   "metadata": {},
   "source": [
    "Now we reset the default graph and see what's left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "familiar-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "drawn-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "understood-karaoke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-shaft",
   "metadata": {},
   "source": [
    "Ok, after diverting a little bit to `tf.reset_default_graph()`, we turn back to autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "timely-campbell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 11.5359\n",
      "(epoch 0100) MSE = 0.8343\n",
      "(epoch 0200) MSE = 0.6230\n",
      "(epoch 0300) MSE = 0.5904\n",
      "(epoch 0400) MSE = 0.5718\n",
      "(epoch 0500) MSE = 0.5586\n",
      "(epoch 0600) MSE = 0.5491\n",
      "(epoch 0700) MSE = 0.5423\n",
      "(epoch 0800) MSE = 0.5373\n",
      "(epoch 0900) MSE = 0.5337\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-insured",
   "metadata": {},
   "source": [
    "The `gradients()` function takes an op (in this case `mse`) and a list of variables (in this case just `theta`), and it creates **a list of ops** (**one per variable**) to compute the gradients of the op with respect to each variable. Indeed, in the code above, the list of variables is just `[theta]`, a list of only one variable, so at the end of the line we take the 0-th element of the returned gradients `[0]`, which is the gradient of `mse` with respect to `theta`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-twenty",
   "metadata": {},
   "source": [
    "### Optimizers, an even more convenient alternative to `tf.gradients()`\n",
    "Replace two lines:\n",
    "- `gradients = ...`\n",
    "- `training_op = ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "desperate-spelling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 7.1328\n",
      "(epoch 0100) MSE = 0.6748\n",
      "(epoch 0200) MSE = 0.5836\n",
      "(epoch 0300) MSE = 0.5664\n",
      "(epoch 0400) MSE = 0.5550\n",
      "(epoch 0500) MSE = 0.5467\n",
      "(epoch 0600) MSE = 0.5407\n",
      "(epoch 0700) MSE = 0.5363\n",
      "(epoch 0800) MSE = 0.5331\n",
      "(epoch 0900) MSE = 0.5308\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "#gradients = tf.gradients(mse, [theta])[0]\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-bumper",
   "metadata": {},
   "source": [
    "One thing also worth noticing is that the cells like the one above **gives a quite different numerical result at each execution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-pressing",
   "metadata": {},
   "source": [
    "We have a number of diff choices for the optimizer, e.g. we could have chosen `MomentumOptimizer` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "opening-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0000) MSE = 6.3637\n",
      "(epoch 0100) MSE = 0.5366\n",
      "(epoch 0200) MSE = 0.5247\n",
      "(epoch 0300) MSE = 0.5243\n",
      "(epoch 0400) MSE = 0.5243\n",
      "(epoch 0500) MSE = 0.5243\n",
      "(epoch 0600) MSE = 0.5243\n",
      "(epoch 0700) MSE = 0.5243\n",
      "(epoch 0800) MSE = 0.5243\n",
      "(epoch 0900) MSE = 0.5243\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "#gradients = tf.gradients(mse, [theta])[0]\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"(epoch {epoch:04d}) MSE = {sess.run(mse):.4f}\")\n",
    "            print(f\"(epoch {epoch:04d}) MSE = {mse.eval():.4f}\")\n",
    "    \n",
    "    final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-replication",
   "metadata": {},
   "source": [
    "If you tried executing above cell (`MomentumOptimizer`) several times, you've probably noticed that it seems to be the only one so far the gives a stable converging minimum of `0.5243`, which seemingly proves the common saying that `MomentumOptimizer` converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-modeling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
