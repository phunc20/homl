{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff in `tf`\n",
    "An toy function\n",
    "$$\n",
    "\\begin{align}\n",
    "  f(w_{1}, w_{2}) &= 3 w_{1}^2 + 2w_{1}w_{2} \\\\\n",
    "  \\nabla f(w_{1}, w_{2}) &= \\begin{pmatrix}\n",
    "    6w_{1} + 2w_{2} \\\\\n",
    "    2w_{1}\n",
    "  \\end{pmatrix} \\\\\n",
    "  \\nabla f(5, 3) &= \\begin{pmatrix}\n",
    "    36 \\\\\n",
    "    10\n",
    "  \\end{pmatrix} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives by Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 + 2*w1*w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of this approach, as indicated by Aurelien, is that, for each partial derivative, say w.r.t. $w_{j}$, we need to evalution the function once at $f(w_{1}, \\ldots, w_{j} + \\epsilon, \\ldots, w_{n})\\,.$\n",
    "\n",
    "Usually for a deep neural network, the number of parameters $w_{j}$ can be large, which means that we need to evaluate some function $f$ many many times. (In the most cases of DNN, $f$ is the cost function $J\\,.$) This can quickly become expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives by `tf.GradientTape`\n",
    "Let's see how we can do this in `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the points after the numbers, they tell tf that these should be recognized as float, not int\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradient = tape.gradient(z, [w1, w2])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, numerically `tf` gives closer results than our calculation by definition. A few of Aurelien's comments on this:\n",
    "\n",
    "> - The precision is only limited by floating-point errors\n",
    "> - only goes through the recoreded computation **once** (in reverse order), no matter how many independant variables there are\n",
    "> - To save memory, only put the strict minimum inside the `with tf.GradientTape() as tape` block. Alternatively, pause recording by creating a `with tape.stop_recording()` block inside the `with tf.GradientTape() as tape` block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `%%timeit` magic to measure the time for running both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 ms ± 29.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps\n",
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697 µs ± 11.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradient = tape.gradient(z, [w1, w2])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tape is automatically erased after you call its `gradient()` method, so you will get an exception if you try\n",
    "to call `gradient()` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dw1 = 36.0\n",
      "type(dz_dw1) = <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "RuntimeError: GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "print(f\"dz_dw1 = {dz_dw1}\")\n",
    "print(f\"type(dz_dw1) = {type(dz_dw1)}\")\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)\n",
    "    print(f\"dz_dw2 = {dz_dw2}\")\n",
    "    print(f\"type(dz_dw2) = {type(dz_dw2)}\")\n",
    "except RuntimeError as err:\n",
    "    #print(f\"{err.__class__}: {err}\")\n",
    "    print(f\"RuntimeError: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error message above suggested, and as Aurelien further explained, if we need to call `gradient()` more than\n",
    "once, we must make the tape persistent and **_delete it after we are done_** to **_free resources_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dw1 = [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>]\n",
      "dz_dw2 = [<tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, [w1])\n",
    "print(f\"dz_dw1 = {dz_dw1}\")\n",
    "dz_dw2 = tape.gradient(z, [w2])\n",
    "print(f\"dz_dw2 = {dz_dw2}\")\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tape will only track operations involving `tf.Variable`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradient = tape.gradient(z, [c1, c2])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can force the tape to **_watch_** any tensors, enabling gradient calculations in this situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradient = tape.gradient(z, [c1, c2])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.GradientTape()` thing we just experiencde is called **_reverse-mode autodiff_**. It only needs to do **_one forward pass_** and **_one backward pass_** in order to get all the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient, Jacobian, Hessian\n",
    "If our `f` is a vector-valued function instead of a scalar-valued function, i.e.\n",
    "\n",
    "$$\n",
    "  f: w \\in \\mathbb{R}^n \\mapsto \\left(f_{1}(w), \\ldots, f_{m}(w)\\right) \\in \\mathbb{R}^m\\,,\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "- `tape.gradient()` will compute the gradient of the sum $\\sum_{i=1}^{m} f_{i}$ w.r.t. the $w_{j}$'s\n",
    "- There is a `tape.jacobian()` method which will compute the Jacobian of $f$, i.e. the gradients of the $f_{i}$'s, **_in parallel_**\n",
    "- It is even possible to compute the Hessian of a scalar-valued function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the above-mentioned computations in `tf` for the following vector-valued function $g\\,,$ and scalar-valued functions $h$:\n",
    "$$\n",
    "\\begin{align}\n",
    "  g:\n",
    "  \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  \\begin{pmatrix}\n",
    "    z_1 \\\\\n",
    "    z_2 \\\\\n",
    "    z_3\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    f(w_1, w_2 + 2) \\\\\n",
    "    f(w_1, w_2 + 5) \\\\\n",
    "    f(w_1, w_2 + 7)\n",
    "  \\end{pmatrix} \\\\\n",
    "  h:\n",
    "  \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  z_1 + z_2 + z_3 =\n",
    "  f(w_1, w_2 + 2) +\n",
    "  f(w_1, w_2 + 5) +\n",
    "  f(w_1, w_2 + 7) \\\\\n",
    "  \\text{Jacobian}(g):\n",
    "  \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  \\begin{pmatrix}\n",
    "    6w_1 + 2(w_2 + 2) & 2w_1 \\\\\n",
    "    6w_1 + 2(w_2 + 5) & 2w_1 \\\\\n",
    "    6w_1 + 2(w_2 + 7) & 2w_1 \\\\\n",
    "  \\end{pmatrix} \\\\\n",
    "  \\nabla h:\n",
    "  \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  \\begin{pmatrix}\n",
    "    18w_1 + 6w_2 + 2(2+5+7) \\\\\n",
    "    6w_1\n",
    "  \\end{pmatrix} \\\\\n",
    "  \\begin{pmatrix}\n",
    "    5 \\\\\n",
    "    3\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  \\begin{pmatrix}\n",
    "    136 \\\\\n",
    "    30\n",
    "  \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "# This should correspond to the promised gradient of the sum (z1 + z2 + z3)\n",
    "gradient = tape.gradient([z1, z2, z3], [w1, w2])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobian =\n",
      "[[40. 10.]\n",
      " [46. 10.]\n",
      " [50. 10.]]\n",
      "gradient = [136.  30.]\n"
     ]
    }
   ],
   "source": [
    "# Let's also verify this numerically\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "jacobian = tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)])\n",
    "gradient = tf.reduce_sum(jacobian, axis=0)\n",
    "del tape\n",
    "print(f\"jacobian =\\n{jacobian}\")\n",
    "print(f\"gradient = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient =\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n",
      "hessian =\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        z = f(w1, w2)\n",
    "    gradient = gradient_tape.gradient(z, [w1, w2])\n",
    "hessian = [hessian_tape.gradient(partial_derivative, [w1, w2])\n",
    "           for partial_derivative in gradient]\n",
    "del hessian_tape\n",
    "print(f\"gradient =\\n{gradient}\")\n",
    "print(f\"hessian =\\n{hessian}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "  \\text{Hessain}(f):\n",
    "  \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2\n",
    "  \\end{pmatrix} &\\mapsto\n",
    "  \\begin{pmatrix}\n",
    "    6 & 2 \\\\\n",
    "    2 & 0 \\\\\n",
    "  \\end{pmatrix} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Rmk.** Just like in Julia, the **_partial derivative of a constant function_** gives **`None`** in `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = (lambda x: tf.constant(3.14159))(w1)\n",
    "\n",
    "tape.gradient(z, w1) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask `tf` to stop tracking gradient from some part of our neural network by using the `tf.stop_gradient()`.\n",
    "\n",
    "During the forward pass, it still computes the part inside `tf.stop_gradient()`, whereas, during the backward\n",
    "pass, it does not let gradient through. (It is **_as if that part equals some constant_**. Note that a constant's derivative equals `None` in `tf`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 105.0\n",
      "gradient = [<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]\n"
     ]
    }
   ],
   "source": [
    "def ff(w1, w2):\n",
    "    return 3*w1**2 + tf.stop_gradient(2*w1*w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = ff(w1, w2)\n",
    "\n",
    "gradient = tape.gradient(z, [w1, w2])\n",
    "print(f\"z = {z}\")\n",
    "print(f\"gradient = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 105.0\n",
      "gradient = [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradient = tape.gradient(z, [w1, w2])\n",
    "print(f\"z = {z}\")\n",
    "print(f\"gradient = {gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"lines.color\": \"white\",\n",
    "    \"patch.edgecolor\": \"white\",\n",
    "    \"text.color\": \"black\",\n",
    "    \"axes.facecolor\": \"black\",\n",
    "    \"axes.edgecolor\": \"lightgray\",\n",
    "    \"axes.labelcolor\": \"white\",\n",
    "    \"axes.titlecolor\": \"white\",\n",
    "    \"xtick.color\": \"white\",\n",
    "    \"ytick.color\": \"white\",\n",
    "    \"grid.color\": \"lightgray\",\n",
    "    \"figure.facecolor\": \"black\",\n",
    "    \"figure.edgecolor\": \"black\",\n",
    "    \"savefig.facecolor\": \"black\",\n",
    "    \"savefig.edgecolor\": \"black\",\n",
    "    \"legend.edgecolor\": \"white\",\n",
    "    \"legend.facecolor\": \"white\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softplus` funciton is\n",
    "\n",
    "- quite similar to `relu`, except that\n",
    "  - it's smoother -- its derivative $z \\mapsto \\frac{e^z}{e^z + 1}$ is continuous on $\\mathbb{R}\\,.$\n",
    "  - `softplus(0)` is not equal to `relu(0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softplus(0.)  # N.B. my_softplus(0) will produce error because of dtype inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "width = 10\n",
    "n_points = 9_000\n",
    "x = np.linspace(-width, width, n_points)\n",
    "y = my_softplus(x).numpy()\n",
    "yy = tf.keras.activations.relu(x).numpy()\n",
    "\n",
    "alpha = 0.5\n",
    "plt.plot(x, y, alpha=alpha, label=\"y = softplus(x)\");\n",
    "plt.plot(x, yy, alpha=alpha, label=\"y = relu(x)\");\n",
    "#plt.title(\"y = my_softplus(x)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the curve of `relu` is the asymptotics of that of `softplus`. Indeed, $\\text{softplus}'(z) = \\frac{e^z}{e^z + 1}$ is an increasing function from $\\frac{1}{2}$ to $1$ as $z$ goes from $0$ to $\\infty\\,.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate `softplus`' gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=nan>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nan`? What happened?\n",
    "\n",
    "Aurelien said,\n",
    "> Autodiff in the above case ends up computing infinity divided by infinity (which returns NaN).\n",
    "\n",
    "Let's use Numpy to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6881171418161356e+43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100.) / np.exp(100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phunc20/.virtualenvs/tf2.2.0-torch1.6.0-py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100., dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phunc20/.virtualenvs/tf2.2.0-torch1.6.0-py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/phunc20/.virtualenvs/tf2.2.0-torch1.6.0-py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100., dtype=np.float32) / np.exp(100., dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6881171418161356e+43"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100., dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100., dtype=np.float64) / np.exp(100., dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.float64, tf.float16, tf.float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.float64, tf.float16, tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=1.0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(100., dtype=tf.float64)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, more precisely speaking, this happens due to **`float32` _overflow_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workaround\n",
    "Another, better, workaround for this is to find a more numerically stable formula for the derivative and then tell `tf` to use the found formula instead. Let's see closer how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\frac{e^z}{e^z + 1}$ causes overflow, its equivalent $\\frac{e^z}{e^z + 1} = \\frac{1}{1 + e^{-z}}$ won't cause overflow. Let's use this formula.\n",
    "\n",
    "To inform `tf` of this, we\n",
    "\n",
    "- rewrite the `softplus` function\n",
    "- have the new function return\n",
    "  - both the old return value of `softplus`\n",
    "  - as well as a function whose input is $\\frac{\\partial J}{\\partial (\\text{so far})}$ and whose output is $\\frac{\\partial J}{\\partial (\\text{so far})} \\cdot \\frac{1}{1 + e^{-z}}\\,,$ as if we are guiding `tf` through the differentiation of this part\n",
    "- finally, we decorate the function with `@tf.custom_gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradient(grad):\n",
    "        return grad / (1 + 1/exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_better_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9999546>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(10.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_better_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.99995464>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(10.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that overflow takes place at `89.` already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=nan>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(89.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(89.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_better_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of changing to this formula is that it even allows way larger `z` without ever worrying about overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(1e38)\n",
    "z.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    xi = my_better_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't convert Python sequence with a value out of range for a double-precision float.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z = tf.Variable(1e39)\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** Could you explain/review the reason for the above error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(tf.where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cheat_softplus(z):\n",
    "    # one can think of tf.where() below as (in C/Julia)\n",
    "    # z > 30.? z : tf.math.log(tf.exp(z) + 1.)\n",
    "    return tf.where(z > 30., z, tf.math.log(tf.exp(z) + 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=nan>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = my_cheat_softplus(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=100.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=inf>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi, my_softplus(100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    xi = (lambda t: t)(z)\n",
    "\n",
    "tape.gradient(xi, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** Could you explain why `my_cheat_softplus()` is not helping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
