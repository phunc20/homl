{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe432a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca955fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/phunc20/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c17603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "shakespeare_text[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690c4f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7f1606ef0950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018e1457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit_on_texts in module keras_preprocessing.text:\n",
      "\n",
      "fit_on_texts(texts) method of keras_preprocessing.text.Tokenizer instance\n",
      "    Updates internal vocabulary based on a list of texts.\n",
      "    \n",
      "    In the case where texts contains lists,\n",
      "    we assume each entry of the lists to be a token.\n",
      "    \n",
      "    Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
      "    \n",
      "    # Arguments\n",
      "        texts: can be a list of strings,\n",
      "            a generator of strings (for memory-efficiency),\n",
      "            or a list of list of strings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.fit_on_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3625a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would happen if we forgot its arg has to be a list?\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28898c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed4f62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20], [6], [9], [8], [3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(\"First\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "469072c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = list(\"First\")\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7672a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20], [6], [9], [8], [3]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7372b56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', 'i', 'r', 's', 't']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da4b62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ad357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79572694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'i', 'r', 's', 't']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20], [6], [9], [8], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30c900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "max_id  # num of distinct characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd5778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"  e t o a i h s r n \\n l d u m y w , c f g b p : k v . ' ; ? ! - j q x z 3 & $\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([range(1, max_id+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7afe50",
   "metadata": {},
   "source": [
    "**Rmk**. To make it visually clearer, `sequences_to_texts()` returns the characters, each separated by an extra space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e127b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e99490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e161dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a7b18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[0]])[0] == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74313772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54580434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map each letter to its ID (0-38 instead of 1-39)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da7117e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a7079c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f0a6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115cd2c",
   "metadata": {},
   "source": [
    "## Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7feaa7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1  # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c523534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(19, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(18, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds_just_flat = dataset.flat_map(lambda ds: ds)\n",
    "for i, tensor in enumerate(ds_just_flat):\n",
    "    if i < 10:\n",
    "        print(tensor)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "830cc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda ds: ds.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09c96805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n",
      "(101,)\n"
     ]
    }
   ],
   "source": [
    "for i, tensor in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(tensor.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa28fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10_000).batch(batch_size)\n",
    "# Recall that the arg inside shuffle() is the buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7aed5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10 21 13 ... 10  5  9]\n",
      " [ 1 12  0 ... 12  5  1]\n",
      " [ 6  1  0 ... 12  0 14]\n",
      " ...\n",
      " [ 7 22  1 ...  4  2  0]\n",
      " [ 0  8  1 ...  0  7  3]\n",
      " [ 1  0 18 ... 13  9  5]], shape=(32, 101), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 4  2  0 ...  0 15  3]\n",
      " [ 0 16  6 ...  9  3  2]\n",
      " [11  0 12 ...  2  3  0]\n",
      " ...\n",
      " [ 3  0 22 ...  1 11 25]\n",
      " [ 7 10  2 ...  6  0  4]\n",
      " [ 9  5 13 ... 13  9  7]], shape=(32, 101), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 9  5 14 ... 11 11  3]\n",
      " [22  5  9 ...  0 20  5]\n",
      " [ 2  6  1 ...  9  0  4]\n",
      " ...\n",
      " [ 0  3 19 ... 15  3 13]\n",
      " [31 18  8 ...  0  2  3]\n",
      " [ 6  3 13 ...  3 19  0]], shape=(32, 101), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset):\n",
    "    if i < 3:\n",
    "        print(batch)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a3c5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56ba75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2 (32, 100) (32, 100)\n",
      "<class 'tuple'> 2 (32, 100) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset):\n",
    "    if i < 2:\n",
    "        print(type(batch), len(batch), batch[0].shape, batch[1].shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c725b4",
   "metadata": {},
   "source": [
    "**(?)** Why the target is a tensor of length `100`, not a single character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08a5f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch:\n",
    "                      (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665e0f7",
   "metadata": {},
   "source": [
    "Let's make sure the correctness of `tf.one_hot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c283f83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
       "array([[23, 25, 22, ..., 31, 22, 27],\n",
       "       [11,  0, 17, ..., 15, 16, 22],\n",
       "       [32, 16, 10, ..., 37, 18, 16],\n",
       "       ...,\n",
       "       [19,  5, 19, ..., 27, 17,  5],\n",
       "       [ 8, 30, 20, ...,  1, 21,  0],\n",
       "       [33, 14, 30, ..., 12, 27, 24]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform(maxval=max_id-1, dtype=tf.int32, shape=(32,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a48ea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 100, 39), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch = tf.random.uniform(maxval=max_id-1, dtype=tf.int32, shape=(32,100))\n",
    "one_hotted = tf.one_hot(X_batch, depth=max_id)\n",
    "one_hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69331c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumsum\n",
      "cumulative_logsumexp\n",
      "reduce_logsumexp\n",
      "reduce_sum\n",
      "segment_sum\n",
      "unsorted_segment_sum\n"
     ]
    }
   ],
   "source": [
    "for f in dir(tf.math):\n",
    "    if \"sum\" in f:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "659af6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    of the entries in `axis`, which must be unique. If `keepdims` is true, the\n",
      "    reduced dimensions are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "    >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "    >>> x.numpy()\n",
      "    array([[1, 1, 1],\n",
      "           [1, 1, 1]], dtype=int32)\n",
      "    >>> # sum all the elements\n",
      "    >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "    >>> tf.reduce_sum(x).numpy()\n",
      "    6\n",
      "    >>> # reduce along the first dimension\n",
      "    >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "    >>> tf.reduce_sum(x, 0).numpy()\n",
      "    array([2, 2, 2], dtype=int32)\n",
      "    >>> # reduce along the second dimension\n",
      "    >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "    >>> tf.reduce_sum(x, 1).numpy()\n",
      "    array([3, 3], dtype=int32)\n",
      "    >>> # keep the original dimensions\n",
      "    >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "    array([[3],\n",
      "           [3]], dtype=int32)\n",
      "    >>> # reduce along both dimensions\n",
      "    >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "    >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "    >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "    >>> # 2 + 2 + 2 = 6\n",
      "    >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "    6\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.math.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0be64b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 100), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(one_hotted, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4079e3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3200.0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(one_hotted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87350cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ddd47e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2 (32, 100, 39) (32, 100)\n",
      "<class 'tuple'> 2 (32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset):\n",
    "    if i < 2:\n",
    "        print(type(batch), len(batch), batch[0].shape, batch[1].shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1defb",
   "metadata": {},
   "source": [
    "## Building and Training the Char-Rnn Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87bd7c",
   "metadata": {},
   "source": [
    "**(?)** Why use `sparse_categorical_crossentropy`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309a359",
   "metadata": {},
   "source": [
    "**(?)** `GRU` with `128` units. What that really mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb6a9e",
   "metadata": {},
   "source": [
    "**(?)** The meaning of `dropout` and `recurrent_dropout` in `keras.layers.GRU()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d97109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\")),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47298267",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc30388",
   "metadata": {},
   "source": [
    "## Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7ed1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)  # equiv. to tf.one_hot(X, depth=max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1cab72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1, 39])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess(\"How are yo\")\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b50a8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 39])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab421a",
   "metadata": {},
   "source": [
    "`[1, 10, 39]` is a shape similar to a batch's shape `(32, 100, 39)` above. Let's try to input a list of two strings.\n",
    "\n",
    "```python\n",
    "X_new = preprocess([\"How are yo\", \"I'm fin\"])\n",
    "X_new.shape\n",
    "----------------------------------------------\n",
    "```\n",
    "```\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-48-1dc23507da9a> in <module>\n",
    "----> 1 X_new = preprocess([\"How are yo\", \"I'm fin\"])\n",
    "      2 X_new.shape\n",
    "\n",
    "<ipython-input-44-113c814f9c80> in preprocess(texts)\n",
    "      1 def preprocess(texts):\n",
    "----> 2     X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "      3     return tf.one_hot(X, max_id)  # equiv. to tf.one_hot(X, depth=max_id)\n",
    "\n",
    "TypeError: unsupported operand type(s) for -: 'list' and 'int'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28e7d2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 4, 17, 1, 5, 9, 2, 1, 16, 4], [6, 28, 15, 1, 20, 6, 10]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"How are yo\", \"I'm fin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45e7093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phunc20/.virtualenvs/homl2e/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([7, 4, 17, 1, 5, 9, 2, 1, 16, 4]),\n",
       "       list([6, 28, 15, 1, 20, 6, 10])], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc7db7",
   "metadata": {},
   "source": [
    "This results in a numpy array of `dtype=object` (more preciesely, lists) because the two lists are of different length. That's why it cannot be subtracted `1` from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e130816a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phunc20/.virtualenvs/homl2e/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[34, 34, 34, 13, 13, 13, 22,  9, 13, 13]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_new)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8eb6b4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94793c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x x x u u u p n u u']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3120a9f",
   "metadata": {},
   "source": [
    "Note that our input was **a single batch** of a sequence of `10` characters, thus so should the output `Y_pred` be of shape `(1, 10)`. Mapped to the corresponding characters, it should give `10` characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "252399a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x x x u u u p n u u'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred + 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29a6b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272f20e",
   "metadata": {},
   "source": [
    "**(?)** Why `Y_pred + 1` before sending into `tokenizer.sequences_to_texts()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afd70888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q q q d d d b r d d']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41509f7e",
   "metadata": {},
   "source": [
    "**(R)** We have to `Y_pred + 1` because the RNN model will only return `[0..38]` as neuron output, but our `tokenizer` has encoded the characters to `[1..39]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3006d",
   "metadata": {},
   "source": [
    "**(?)** `predict()` vs `predict_class()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0448755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77611584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "95f159c0",
   "metadata": {},
   "source": [
    "X_new = [preprocess([s]) for s in [\"How are yo\", \"I'm fin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e829336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b72a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
