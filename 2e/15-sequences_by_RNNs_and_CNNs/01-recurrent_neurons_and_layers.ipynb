{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0a5d0a",
   "metadata": {},
   "source": [
    "## Clarification of A Recurrent Neuron and A Layer of Recurrent Neurons\n",
    "Actually the figures `15-1` and `15-2` expresses quite accurately what each vector's dimension is, namely when it is **not boldface**,\n",
    "it concerns a **scalar** (like the $y, y_{(t-3)}\\,, y_{(t-2)}\\,, y_{(t-1)}\\,, y_{(t)}$ in figure `15-1`.) And when it is **boldface**,\n",
    "it concerns a **vector** (like the $\\mathbf{y}, \\mathbf{y}_{(0)}\\,, \\mathbf{y}_{(1)}\\,, \\mathbf{y}_{(2)}$ in figure `15-2`.)\n",
    "![](./figs/fig.15-1.png)\n",
    "![](./figs/fig.15-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4af21e",
   "metadata": {},
   "source": [
    "More explicitly speaking,\n",
    "\n",
    "- a recurrent neuron's output is a scalar\n",
    "- a layer of recurrent neurons is a cooperative unit of multiple recurrent neurons. And its output is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb728a",
   "metadata": {},
   "source": [
    "## Recurrent Neuron\n",
    "We will have\n",
    "\n",
    "- $y_{(t)} \\in \\mathbb{R}$ for all time $t$.\n",
    "- $\\mathbf{x}_{(t)} \\in \\mathbb{R}^{n_{\\,\\text{inputs}}}\\;\\;$ for all time $t$.\n",
    "- A single neuron's parameters are vectors $\\mathbf{w_x} \\in \\mathbb{R}^{n_{\\,\\text{inputs}}}\\;\\;$ and scalars $w_y \\in \\mathbb{R}, b \\in \\mathbb{R}$\n",
    "- The formula connecting all these together is the following: $$y_{(t)} = \\mathbf{w_x} \\cdot \\mathbf{x}_{(t)} + w_y y_{(t-1)} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f88d1e",
   "metadata": {},
   "source": [
    "## A Layer of Recurrent Neurons (To be edited!!!)\n",
    "We will have\n",
    "\n",
    "For a single instance,\n",
    "- $\\mathbf{y}_{(t)} \\in \\mathbb{R}^{n_{\\,\\text{neurons}}}\\;\\;\\;$ for all time $t$.\n",
    "- $\\mathbf{x}_{(t)} \\in \\mathbb{R}^{n_{\\,\\text{inputs}}}\\;\\;$ for all time $t$.\n",
    "- Parameters: Matrices $W_{\\mathbf{x}} \\in \\mathbb{R}^{n_{\\,\\text{inputs}}}\\;\\;$ and scalars $w_y \\in \\mathbb{R}, b \\in \\mathbb{R}$\n",
    "- The formula connecting all these together is the following: $$y_{(t)} = \\mathbf{w_x} \\cdot \\mathbf{x}_{(t)} + w_y y_{(t-1)} + b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef0cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f69f5b53",
   "metadata": {},
   "source": [
    "help(keras.layers.SimpleRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905e6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleRNN_layer = keras.layers.SimpleRNN(5, input_shape=(28*28,))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51d1e7bd",
   "metadata": {},
   "source": [
    "[s for s in dir(simpleRNN_layer) if not s.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d388da91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleRNN_layer.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbb489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1695353f",
   "metadata": {},
   "source": [
    "## Memory Cells\n",
    "In a more sophisticated setting, there is also sth called **hidden state**, usually noted as $\\mathbf{h}_{(t)}\\,.$\n",
    "And the common practice is\n",
    "\n",
    "- let $\\mathbf{h}_{(t)} = f(\\mathbf{h}_{(t-1)}\\,, \\mathbf{x}_{(t)}\\,)$ for some function $f$\n",
    "- let $\\mathbf{y}_{(t)} = g(\\mathbf{h}_{(t-1)}\\,, \\mathbf{x}_{(t)}\\,)$ for some function $g$.\n",
    "\n",
    "In what we discussed above (for the simplest case), the output $\\mathbf{y}_{(t)}$ plays the role of a hidden state $\\mathbf{h}_{(t)}$ and there was no $\\mathbf{h}_{(t)}$. But further in this chapter, we will encounter more sophisticated RNNs which do make use of hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee51de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d8731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
